{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0502feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy.io as scio\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b81aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d81e66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hone\\AppData\\Local\\Temp/ipykernel_8508/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab8383ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, regularizers, constraints, initializers\n",
    "\n",
    "class HGT_Layer_dynamic(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 m,\n",
    "                 activation=lambda x: x,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        super(HGT_Layer_dynamic, self).__init__()\n",
    "\n",
    "        self.m = m #要降成的维度\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        \n",
    "        \n",
    "        self.convolutions = []\n",
    "  \n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #print(input_shape)\n",
    "\n",
    "        \"\"\"\n",
    "        HGT_Layer has three inputs : [shape(AnCount,An), shape(X_count,X),shape(Z_count,Z)]\n",
    "        X is self_feature,Z is another node type feature\n",
    "        \n",
    "        \"\"\"\n",
    "        Adj_set=input_shape[0]  #输入一个list，元素为一个邻接矩阵\n",
    "        X_shape  =input_shape[1]\n",
    "        Z_shape  =input_shape[2]\n",
    "        \n",
    "        #设置Q,K,V Q权重矩阵,因为输入的是多个邻接矩阵，所以对应多个Q,K,V,使用list分别存储\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        self.weight_Q=self.add_weight(name=\"weight_Q\",\n",
    "                        shape=(X_shape[1], self.m),\n",
    "                        initializer=self.kernel_initializer,\n",
    "                        trainable=True)\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        self.weight_K_T=self.add_weight(name=\"weight_K_T\",\n",
    "                        shape=(self.m,Z_shape[1]),\n",
    "                        initializer=self.kernel_initializer,\n",
    "                        trainable=True)\n",
    "            \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.weight_V=self.add_weight(name=\"weight_V\",\n",
    "                        shape=(Z_shape[1], self.m),\n",
    "                        initializer=self.kernel_initializer,\n",
    "                        trainable=True)\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "           \n",
    "        self.weight_ATT=self.add_weight(name=\"weight_V\",\n",
    "                        shape=(self.m, self.m),\n",
    "                        initializer=self.kernel_initializer,\n",
    "                        trainable=True)\n",
    "            \n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\" HGT_Layer has three inputs : [An, X，Z]\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X = inputs[1]\n",
    "        self.Z = inputs[2]\n",
    "        self.Z_T=tf.transpose(self.Z)\n",
    "        self.An=inputs[0] \n",
    "        \n",
    "        if self.An.shape[0]!=self.X.shape[0]:     \n",
    "            self.An=tf.sparse.transpose(self.An)\n",
    "\n",
    "\n",
    "            \n",
    "        ##################################################################\n",
    "        self.Q=tf.matmul(self.X,self.weight_Q)\n",
    "        self.K_T=tf.matmul(self.weight_K_T,self.Z_T)\n",
    "        self.V=tf.matmul(self.Z,self.weight_V)\n",
    "        \n",
    "        ##################################################################\n",
    "        Att=tf.matmul(self.Q,self.weight_ATT)\n",
    "        Att=tf.matmul(Att,self.K_T)\n",
    "        Att=Att*self.An  #An是一个邻接矩阵，做一个哈达玛积,是一个sparse.Tensor\n",
    "        Att=tf.sparse.softmax(Att)\n",
    "        ##################################################################\n",
    "\n",
    "        output=tf.sparse.sparse_dense_matmul(Att,self.V)\n",
    "            \n",
    "        if self.activation:\n",
    "            output=self.activation(output)\n",
    "        ##################################################################\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f20fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 att_size,\n",
    "                 activation=lambda x: x,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',):\n",
    "        super(Self_Attention, self).__init__()\n",
    "        self.activation = activations.get(activation)\n",
    "        self.att_size=att_size\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        weight_att_shape_x=input_shape[-1]\n",
    "        weight_att_shape_y=self.att_size\n",
    "        \n",
    "        self.weight_att=self.add_weight(name=\"weight_to_att\",\n",
    "                               shape=(weight_att_shape_x, weight_att_shape_y),\n",
    "                               initializer=self.kernel_initializer, \n",
    "                               trainable=True)\n",
    "        \n",
    "        self.bias=self.add_weight(name=\"bias\",\n",
    "                               shape=(weight_att_shape_y,),\n",
    "                               initializer=self.bias_initializer,\n",
    "                               trainable=True)\n",
    "        \n",
    "        self.weight_att_u=self.add_weight(name=\"weight_to_att_u\",\n",
    "                               shape=(weight_att_shape_y, 1),\n",
    "                               initializer=self.kernel_initializer,\n",
    "                               trainable=True)\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        self.X = inputs\n",
    "        \n",
    "        WX=tf.matmul(self.X,self.weight_att)\n",
    "        V=tf.tanh(tf.add(WX,self.bias))\n",
    "        VU=tf.matmul(V,self.weight_att_u)\n",
    "        alphas=tf.nn.softmax(VU,axis=0) \n",
    "        \n",
    "        outputs=tf.multiply(alphas,self.X)\n",
    "        outputs_att=tf.reduce_sum(outputs,axis=0) \n",
    "        \n",
    "        if self.activation:\n",
    "            outputs_att=self.activation(outputs_att)\n",
    "        \n",
    "#         print('inputs.shape=',inputs.shape)\n",
    "#         print('VU.shape=',VU.shape)\n",
    "#         print('alphas.shape=',alphas.shape)\n",
    "        alphas_new=tf.transpose(tf.squeeze(alphas, axis=-1)) #需要改变attention形状\n",
    "\n",
    "        return outputs_att,alphas_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "742fac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 稀疏矩阵转稀疏张量\n",
    "def sp_matrix_to_sp_tensor(M):\n",
    "    if not isinstance(M, sp.csr.csr_matrix):\n",
    "        M = M.tocsr()\n",
    "    # 获取非0元素坐标\n",
    "    row, col = M.nonzero()\n",
    "    # SparseTensor参数：二维坐标数组，数据，形状\n",
    "    X = tf.SparseTensor(np.mat([row, col]).T, M.data, M.shape)\n",
    "    X = tf.cast(X, tf.float32)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c894174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这部分改成Yelp的 \n",
    "\n",
    "class Multi_HGT_Model(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 Muilt_BB,\n",
    "                 Muilt_BU,\n",
    "                 Muilt_BS,\n",
    "                 Muilt_BL,\n",
    "                 Muilt_BR,\n",
    "                 \n",
    "                 Muilt_FB,\n",
    "                 Muilt_FU,\n",
    "                 Muilt_FS,\n",
    "                 Muilt_FL,\n",
    "                 Muilt_FR,\n",
    "    \n",
    "                 output_size=3): \n",
    "\n",
    "        # 调用父类__init__()方法\n",
    "        super(Multi_HGT_Model, self).__init__()\n",
    "        \n",
    "        self.Muilt_BB=Muilt_BB\n",
    "        self.Muilt_BU=Muilt_BU\n",
    "        self.Muilt_BS=Muilt_BS\n",
    "        self.Muilt_BL=Muilt_BL\n",
    "        self.Muilt_BR=Muilt_BR\n",
    "        \n",
    "        \n",
    "        self.Muilt_FB =Muilt_FB\n",
    "        self.Muilt_FU =Muilt_FU\n",
    "        self.Muilt_FS =Muilt_FS\n",
    "        self.Muilt_FL =Muilt_FL\n",
    "        self.Muilt_FR =Muilt_FR\n",
    "     \n",
    "        \n",
    "     \n",
    "        \n",
    "        #同质图\n",
    "        self.HGT_layer_list=[]\n",
    "        for i in range(len(self.Muilt_BB)):\n",
    "            self.HGT_layer_list.append(\n",
    "                HGT_Layer_dynamic(64,activation=tf.keras.activations.gelu)\n",
    "            )\n",
    "            \n",
    "        #二部图\n",
    "        self.HGT_layer_list_bio_BU=[]\n",
    "        for i in range(len(self.Muilt_BU)):\n",
    "            self.HGT_layer_list_bio_BU.append(\n",
    "                HGT_Layer_dynamic(64,activation=tf.keras.activations.gelu)\n",
    "            )\n",
    "            \n",
    "        self.HGT_layer_list_bio_BS=[]\n",
    "        for i in range(len(self.Muilt_BS)):\n",
    "            self.HGT_layer_list_bio_BS.append(\n",
    "                HGT_Layer_dynamic(64,activation=tf.keras.activations.gelu)\n",
    "            )\n",
    "        \n",
    "        self.HGT_layer_list_bio_BL=[]\n",
    "        for i in range(len(self.Muilt_BL)):\n",
    "            self.HGT_layer_list_bio_BL.append(\n",
    "                HGT_Layer_dynamic(64,activation=tf.keras.activations.gelu)\n",
    "           )\n",
    "            \n",
    "        self.HGT_layer_list_bio_BR=[]\n",
    "        for i in range(len(self.Muilt_BR)):\n",
    "            self.HGT_layer_list_bio_BR.append(\n",
    "                HGT_Layer_dynamic(64,activation=tf.keras.activations.gelu)\n",
    "            )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    " \n",
    "        \n",
    "        \n",
    "        #注意力\n",
    "        self.att_layer = Self_Attention(32,activation=tf.keras.activations.gelu)\n",
    "\n",
    "\n",
    "        #Decoder部分\n",
    "        self.dense=tf.keras.layers.Dense(output_size,activation='softmax')\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def call(self,input_x_id, training=False,dropout=0.):\n",
    "        # 输入数据\n",
    "        \n",
    "        #同质网络I 编码过程\n",
    "        \n",
    "        _h1_for_A_list=[]\n",
    "        for i in range(len(self.Muilt_BB)):\n",
    "            _h1_for_A_list.append(\n",
    "                self.HGT_layer_list[i]([self.Muilt_BB[i], self.Muilt_FB,self.Muilt_FB])   \n",
    "            \n",
    "            )\n",
    "       \n",
    "\n",
    "\n",
    "          #二部图BX 编码过程\n",
    "        _h2_for_A_list=[]\n",
    "        for i in range(len(self.Muilt_BU)):\n",
    "            _h2_for_A_list.append(\n",
    "                self.HGT_layer_list_bio_BU[i]([self.Muilt_BU[i], self.Muilt_FB,self.Muilt_FU])   \n",
    "            \n",
    "            )\n",
    "            \n",
    "        _h3_for_A_list=[]\n",
    "        for i in range(len(self.Muilt_BS)):\n",
    "            _h3_for_A_list.append(\n",
    "                self.HGT_layer_list_bio_BS[i]([self.Muilt_BS[i], self.Muilt_FB,self.Muilt_FS])   \n",
    "            \n",
    "            )\n",
    "            \n",
    "        _h4_for_A_list=[]\n",
    "        for i in range(len(self.Muilt_BL)):\n",
    "            _h4_for_A_list.append(\n",
    "                self.HGT_layer_list_bio_BL[i]([self.Muilt_BL[i], self.Muilt_FB,self.Muilt_FL])   \n",
    "            \n",
    "            )\n",
    "            \n",
    "        _h5_for_A_list=[]\n",
    "        for i in range(len(self.Muilt_BR)):\n",
    "            _h5_for_A_list.append(\n",
    "                self.HGT_layer_list_bio_BR[i]([self.Muilt_BR[i], self.Muilt_FB,self.Muilt_FR])   \n",
    "            \n",
    "            )\n",
    "            \n",
    "        \n",
    "       \n",
    "\n",
    "        _h_for_A_list_set=tf.stack(_h1_for_A_list+_h2_for_A_list+_h3_for_A_list+_h4_for_A_list+_h5_for_A_list,\n",
    "                                    axis=0)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "        _h7_for_A,att_for_A=self.att_layer(_h_for_A_list_set) \n",
    "\n",
    "\n",
    "        #解码层\n",
    "\n",
    "\n",
    "        output=self.dense(_h7_for_A) \n",
    "\n",
    "     \n",
    "        \n",
    "        \n",
    "        ###########################################################################\n",
    "        output_list=[]\n",
    "        for idx in input_x_id:\n",
    "            output_list.append(output[idx]) #返回指定id的所有向量\n",
    "       #########################################################################     \n",
    "\n",
    "       \n",
    "        att=[att_for_A,]\n",
    "        \n",
    "        \n",
    "\n",
    "        embedding=_h7_for_A\n",
    "        \n",
    "        embedding_list=[]\n",
    "        for idx in input_x_id:\n",
    "            embedding_list.append(embedding[idx])\n",
    "            \n",
    "        output=[output_list]    \n",
    "        return output,att,embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d4a0e",
   "metadata": {},
   "source": [
    "# 加入数据测试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b07cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取文件夹下所有文件名\n",
    "import os\n",
    "path=r'C:\\Users\\Hone\\Desktop\\Experimental_code\\Yelp-2\\计算的各个元路径矩阵'\n",
    "name_list=[]\n",
    "filelist=os.listdir(path)\n",
    "for filename in filelist:\n",
    "    name_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "965bad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#取出矩阵的名字\n",
    "matrix_name_list=[]\n",
    "for name in name_list:\n",
    "    matrix_name_list.append(name.split('.')[0].split('_matrix_')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f15551b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BLB',\n",
       " 'BLBRB',\n",
       " 'BLBSB',\n",
       " 'BLBUB',\n",
       " 'BRB',\n",
       " 'BRBL',\n",
       " 'BRBSB',\n",
       " 'BS',\n",
       " 'BSB',\n",
       " 'BSBL',\n",
       " 'BSBR',\n",
       " 'BSBU',\n",
       " 'BUB',\n",
       " 'BUBL',\n",
       " 'BUBR',\n",
       " 'BUBRB',\n",
       " 'BUBSB',\n",
       " 'BUBU',\n",
       " 'BUBUB',\n",
       " 'LB',\n",
       " 'RB',\n",
       " 'RBLB',\n",
       " 'SBLB',\n",
       " 'SBRB',\n",
       " 'SBUB',\n",
       " 'UB',\n",
       " 'UBLB',\n",
       " 'UBRB']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3fff060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matrix_from_file(path,key_name):\n",
    "    filelist=os.listdir(path)\n",
    "    \n",
    "    for name in filelist:\n",
    "        if key_name==name.split('.')[0].split('_matrix_')[1]:\n",
    "            Matrix = sp.load_npz(path+'\\\\'+name) #读取\n",
    "            return Matrix\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "\n",
    "    print('cant find %s in %s' %key_name %path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8a19964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#需要将矩阵按照类型进行区分，放进不同的dict\n",
    "data_BB={}\n",
    "data_BU={}\n",
    "data_BS={}\n",
    "data_BL={}\n",
    "data_BR={}\n",
    "\n",
    "\n",
    "\n",
    "for pathname in matrix_name_list:\n",
    "    if pathname[0]==pathname[-1]: #同质邻接矩阵\n",
    "        data_BB[pathname]=find_matrix_from_file(path,pathname) #输入目标位置文件，与指定文件名，返回该稀疏矩阵\n",
    "    elif pathname[0]=='U' or pathname[-1]=='U':\n",
    "        data_BU[pathname]=find_matrix_from_file(path,pathname) \n",
    "    elif pathname[0]=='S' or pathname[-1]=='S':\n",
    "        data_BS[pathname]=find_matrix_from_file(path,pathname) \n",
    "    elif pathname[0]=='L' or pathname[-1]=='L':\n",
    "        data_BL[pathname]=find_matrix_from_file(path,pathname)\n",
    "    elif pathname[0]=='R' or pathname[-1]=='R':\n",
    "        data_BR[pathname]=find_matrix_from_file(path,pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc972fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paths_name_BB=[keyword for keyword in data_BB]\n",
    "Paths_name_BU=[keyword for keyword in data_BU]\n",
    "Paths_name_BS=[keyword for keyword in data_BS]\n",
    "Paths_name_BL=[keyword for keyword in data_BL]\n",
    "Paths_name_BR=[keyword for keyword in data_BR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf42b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paths_name_BB=['BUB','BLB','BSB','BRB']\n",
    "Paths_name_BU=['UB']\n",
    "Paths_name_BS=['BS']\n",
    "Paths_name_BL=['LB']\n",
    "Paths_name_BR=['RB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c2a8f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "BB_matrix=[]\n",
    "for name in Paths_name_BB:\n",
    "    BB_matrix.append(data_BB[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ed5f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_list(name,data_dict):\n",
    "    matepath_adj_list=[]\n",
    "    for i in name:\n",
    "        matepath_adj_list.append(sp_matrix_to_sp_tensor(data_dict[i])) #转tensor.sparse\n",
    "    return matepath_adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7233a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BB_matrix=adj_list(Paths_name_BB,data_BB)\n",
    "BU_matrix=adj_list(Paths_name_BU,data_BU)\n",
    "BS_matrix=adj_list(Paths_name_BS,data_BS)\n",
    "BL_matrix=adj_list(Paths_name_BL,data_BL)\n",
    "BR_matrix=adj_list(Paths_name_BR,data_BR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ac8ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ed501db",
   "metadata": {},
   "outputs": [],
   "source": [
    "FB=np.random.rand(2614,128)\n",
    "FU=np.random.rand(1286,128)\n",
    "FS=np.random.rand(2,128)\n",
    "FL=np.random.rand(9,128)\n",
    "FR=np.random.rand(2,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e64f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edd30b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Multi_HGT_Model(BB_matrix,\n",
    "                           BU_matrix,\n",
    "                           BS_matrix,\n",
    "                           BL_matrix,\n",
    "                           BR_matrix,\n",
    "                           \n",
    "                           FB,\n",
    "                           FU,\n",
    "                           FS,\n",
    "                           FL,\n",
    "                           FR,\n",
    "                           \n",
    "                           output_size=3\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e178765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入训练id集合\n",
    "import random\n",
    "\n",
    "index_list_for_FA=[i for i in range(FB.shape[0])] #需要随机化\n",
    "random.shuffle(index_list_for_FA) #index乱序\n",
    "    \n",
    "a_part=int(len(index_list_for_FA)/10)\n",
    "train_idx_for_FB = index_list_for_FA[:int(a_part*2)]  # 2 1 7\n",
    "val_idx_for_FB = index_list_for_FA[ int(a_part*2)+1:int(a_part*3)]\n",
    "test_idx_for_FB = index_list_for_FA[ int(a_part*3)+1:len(index_list_for_FA)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcb66b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "output,att,embedding=model(train_idx_for_FB,training=True,dropout=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "726ecf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"multi_hgt__model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hgt__layer_dynamic (HGT_Lay  multiple                 28672     \n",
      " er_dynamic)                                                     \n",
      "                                                                 \n",
      " hgt__layer_dynamic_1 (HGT_L  multiple                 28672     \n",
      " ayer_dynamic)                                                   \n",
      "                                                                 \n",
      " hgt__layer_dynamic_2 (HGT_L  multiple                 28672     \n",
      " ayer_dynamic)                                                   \n",
      "                                                                 \n",
      " hgt__layer_dynamic_3 (HGT_L  multiple                 28672     \n",
      " ayer_dynamic)                                                   \n",
      "                                                                 \n",
      " hgt__layer_dynamic_4 (HGT_L  multiple                 28672     \n",
      " ayer_dynamic)                                                   \n",
      "                                                                 \n",
      " hgt__layer_dynamic_5 (HGT_L  multiple                 28672     \n",
      " ayer_dynamic)                                                   \n",
      "                                                                 \n",
      " hgt__layer_dynamic_6 (HGT_L  multiple                 28672     \n",
      " ayer_dynamic)                                                   \n",
      "                                                                 \n",
      " hgt__layer_dynamic_7 (HGT_L  multiple                 28672     \n",
      " ayer_dynamic)                                                   \n",
      "                                                                 \n",
      " self__attention (Self_Atten  multiple                 2112      \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 231,683\n",
      "Trainable params: 231,683\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a3ef64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#标签\n",
    "data_label=sp.load_npz(r'C:\\Users\\Hone\\Desktop\\Experimental_code\\Yelp-2\\sparse_matrix_business_category.npz') #读取\n",
    "data_label=data_label.todense()\n",
    "data_label=np.array(data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13e14421",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam() #learning_rate=0.01\n",
    "loss_func=tf.keras.losses.CategoricalCrossentropy()\n",
    "#这个loss函数还不够全，还得加入L2正则化\n",
    "\n",
    "\n",
    "train_loss=tf.keras.metrics.Mean('train_loss') #计算loss的均值\n",
    "train_acc_I=tf.keras.metrics.CategoricalAccuracy('train_accuracy_I')\n",
    "\n",
    "\n",
    "test_loss=tf.keras.metrics.Mean('test_loss')\n",
    "test_acc_I=tf.keras.metrics.CategoricalAccuracy('test_accuracy_I')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa338531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model,id_x,labels_x,dropout=0.):  #一个set为一个batch的量，\n",
    "    #tf.GradientTape()记录运算过程的变量梯度\n",
    "    \n",
    "    with tf.GradientTape() as t:\n",
    "        output_list,att,embedding=model(input_x_id=id_x,training=True,dropout=dropout)    #计算一个batch的预测值   #!!!\n",
    "     \n",
    "\n",
    "\n",
    "        loss_step_1=loss_func(labels_x,output_list[0])   #计算一个batch的损失函数\n",
    "        loss_step=loss_step_1\n",
    "        \n",
    " \n",
    "    grads=t.gradient(loss_step,model.trainable_variables)  #更新变量的梯度\n",
    "    \n",
    "    \n",
    "    #利用跟新的梯度计算变量，model.trainable_variables表示网络中所有的变量\n",
    " \n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_variables))  \n",
    "    train_loss(loss_step)  #传入计算一个epoch累计loss函数中\n",
    "    train_acc_I(labels_x,output_list) #传入计算一个epoch累计accuracy函数中\n",
    "    \n",
    "    \n",
    "    return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00c0ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model,id_x,labels_x): \n",
    "    output_list,att,embedding=model(input_x_id=id_x,training=False,dropout=0.) \n",
    "    \n",
    "    loss_step_1=loss_func(labels_x,output_list[0])   #计算一个batch的损失函数\n",
    "    #loss_step_2=loss_func(labels_x,output_list[1])   \n",
    "    loss_step=loss_step_1\n",
    "    \n",
    "    test_loss(loss_step)\n",
    "    test_acc_I(labels_x,output_list) #传入计算一个epoch累计accuracy函数中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a016980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_by_n(list_collection, n):\n",
    "    \"\"\"\n",
    "    将集合均分，每份n个元素\n",
    "    :param list_collection:\n",
    "    :param n:\n",
    "    :return:返回的结果为评分后的每份可迭代对象\n",
    "    \"\"\"\n",
    "    for i in range(0, len(list_collection), n):\n",
    "        yield list_collection[i: i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b8002fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "          BB_matrix,\n",
    "          BU_matrix,\n",
    "          BS_matrix,\n",
    "          BL_matrix,\n",
    "          BR_matrix,\n",
    "    \n",
    "          FB,\n",
    "          FU,\n",
    "          FS,\n",
    "          FL,\n",
    "          FR,\n",
    "     \n",
    "    \n",
    "          epoch_time,  \n",
    "          batch_size,\n",
    "          \n",
    "          \n",
    "          id_x_set, \n",
    "          id_x_set_for_val,\n",
    "          id_x_set_for_test,\n",
    "          \n",
    "  \n",
    "          label_set_I,\n",
    "          output_size=3,\n",
    "          dropout=0.\n",
    "         ): #需要给要训练的集合和验证集合\n",
    "    \n",
    "    model=Multi_HGT_Model(BB_matrix,\n",
    "                               BU_matrix,\n",
    "                               BS_matrix,\n",
    "                               BL_matrix,\n",
    "                               BR_matrix,\n",
    "                               \n",
    "                               FB,\n",
    "                               FU,\n",
    "                               FS,\n",
    "                               FL,\n",
    "                               FR,\n",
    "                               \n",
    "                               output_size=output_size\n",
    "                               )\n",
    "\n",
    "    best_acc_result=0\n",
    "    for epoch in range(epoch_time):\n",
    "        \n",
    "        #training\n",
    "        batch_idx_train=[] #获取一个batch的数据\n",
    "        for i in split_list_by_n(id_x_set,batch_size): #split_list_by_n这个函数\n",
    "            batch_idx_train.append(i)  \n",
    "                     \n",
    "        #创建batch_label矩阵\n",
    "\n",
    "        \n",
    "        for a_batch_x in batch_idx_train: \n",
    "            \n",
    "            batch_label_list_x=[]\n",
    "\n",
    "            for id_x in a_batch_x:\n",
    "                batch_label_list_x.append(label_set_I[id_x]) \n",
    "                \n",
    "    \n",
    "\n",
    "            att=train_step(model,\n",
    "                           id_x=a_batch_x,\n",
    "                           labels_x=batch_label_list_x,\n",
    "                           dropout=dropout) \n",
    "            print('一个train_batch训练完毕')\n",
    "                           \n",
    "        print('Epoch {} ，train_loss={:.3f}，train_acc_I={:.3f}'.format(epoch,\n",
    "                                                                        train_loss.result(),\n",
    "                                                                        train_acc_I.result())) \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #validating\n",
    "        batch_idx_val=[] #获取一个batch的数据\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in split_list_by_n(id_x_set_for_val,batch_size):\n",
    "            batch_idx_val.append(i)\n",
    "            \n",
    "\n",
    "            \n",
    "        for a_batch_x in batch_idx_val:\n",
    "            batch_label_list_x=[]\n",
    "            \n",
    "            for id_x in a_batch_x:\n",
    "                batch_label_list_x.append(label_set_I[id_x]) #获取一个batch的所有label\n",
    "\n",
    "                \n",
    "            \n",
    "            test_step(model,id_x=a_batch_x, labels_x=batch_label_list_x)\n",
    "            print('一个val_batch验证完毕')\n",
    "            \n",
    "        print('val_loss={:.3f}，val_accuracy_I={:.3f},best_accuracy={:.3f}'.format(test_loss.result(),test_acc_I.result(),best_acc_result)\n",
    "             )\n",
    "        \n",
    "        ####################################################################################\n",
    "        #保存结果最佳的embedding\n",
    "        test_acc=float(test_acc_I.result())\n",
    "        if test_acc>best_acc_result:\n",
    "            \n",
    "            best_acc_result=test_acc\n",
    "           \n",
    "            \n",
    "            \n",
    "            if best_acc_result>0.6:\n",
    "                \n",
    "                #需要返回，train数据载入得到的att\n",
    "                att=train_step(model,\n",
    "                    id_x=id_x_set,\n",
    "                    labels_x=label_set_I[id_x_set]) \n",
    "    \n",
    "                #输出test数据集的embedding \n",
    "                #输入test数据集，输出test数据集的embedding\n",
    "                _,att_test,embedding=model(input_x_id=id_x_set_for_test,\n",
    "                          training=False) \n",
    "                #还需要一个对应的label矩阵\n",
    "                #从label_set_I中取出对应id的值，重新组成一个新的矩阵\n",
    "                embedding_list=[]\n",
    "                new_label_matrix_list=[]\n",
    "                for i in id_x_set_for_test:\n",
    "                    new_label_matrix_list.append(label_set_I[i])\n",
    "                    embedding_list.append(embedding[i])\n",
    "                    new_label_matrix=np.array(new_label_matrix_list)\n",
    "    \n",
    "                #保存embedding和对应的label矩阵\n",
    "                np.save(r'C:\\Users\\Hone\\Desktop\\My_model-DATA\\Yelp\\embedding_of_test_data_HGT',np.array(embedding_list))\n",
    "                np.save(r'C:\\Users\\Hone\\Desktop\\My_model-DATA\\Yelp\\label_matrix_of_test_data_HGT',new_label_matrix)\n",
    "                print('表示向量保存完毕！')\n",
    "\n",
    "        train_loss.reset_states()\n",
    "        train_acc_I.reset_states()\n",
    "\n",
    "        \n",
    "        test_loss.reset_states()\n",
    "        test_acc_I.reset_states()\n",
    "        ####################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "    #循环结束,对test数据验证\n",
    "    batch_idx_test = []  # 获取一个batch的数据\n",
    "\n",
    "    \n",
    "    for i in split_list_by_n(id_x_set_for_test, batch_size):\n",
    "        batch_idx_test.append(i)\n",
    "\n",
    "    \n",
    "\n",
    "    for a_batch_x in batch_idx_test:\n",
    "        batch_label_list_x = []\n",
    "\n",
    "        \n",
    "        for id_x in a_batch_x:\n",
    "            batch_label_list_x.append(label_set_I[id_x])  # 获取一个batch的所有label\n",
    "    \n",
    "        \n",
    "\n",
    "        test_step(model,id_x=a_batch_x,labels_x=batch_label_list_x)\n",
    "    print('End of model training\\n'\n",
    "          'test_loss={:.3f}，test_accuracy_I={:.3f}'.format(test_loss.result(),\n",
    "                                                          test_acc_I.result()))\n",
    "    \n",
    "\n",
    "    test_loss.reset_states()\n",
    "    test_acc_I.reset_states()\n",
    "\n",
    "          \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "          \n",
    "    return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bee1255e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一个train_batch训练完毕\n",
      "Epoch 0 ，train_loss=1.122，train_acc_I=0.186\n",
      "一个val_batch验证完毕\n",
      "val_loss=1.073，val_accuracy_I=0.431,best_accuracy=0.000\n",
      "一个train_batch训练完毕\n",
      "Epoch 1 ，train_loss=1.071，train_acc_I=0.418\n",
      "一个val_batch验证完毕\n",
      "val_loss=1.028，val_accuracy_I=0.442,best_accuracy=0.431\n",
      "一个train_batch训练完毕\n",
      "Epoch 2 ，train_loss=1.026，train_acc_I=0.425\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.983，val_accuracy_I=0.538,best_accuracy=0.442\n",
      "一个train_batch训练完毕\n",
      "Epoch 3 ，train_loss=0.982，train_acc_I=0.567\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.937，val_accuracy_I=0.738,best_accuracy=0.538\n",
      "表示向量保存完毕！\n",
      "一个train_batch训练完毕\n",
      "Epoch 4 ，train_loss=0.886，train_acc_I=0.743\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.846，val_accuracy_I=0.738,best_accuracy=0.738\n",
      "一个train_batch训练完毕\n",
      "Epoch 5 ，train_loss=0.838，train_acc_I=0.743\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.804，val_accuracy_I=0.738,best_accuracy=0.738\n",
      "一个train_batch训练完毕\n",
      "Epoch 6 ，train_loss=0.793，train_acc_I=0.743\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.767，val_accuracy_I=0.738,best_accuracy=0.738\n",
      "一个train_batch训练完毕\n",
      "Epoch 7 ，train_loss=0.752，train_acc_I=0.743\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.733，val_accuracy_I=0.738,best_accuracy=0.738\n",
      "一个train_batch训练完毕\n",
      "Epoch 8 ，train_loss=0.715，train_acc_I=0.743\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.705，val_accuracy_I=0.738,best_accuracy=0.738\n",
      "一个train_batch训练完毕\n",
      "Epoch 9 ，train_loss=0.683，train_acc_I=0.745\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.685，val_accuracy_I=0.738,best_accuracy=0.738\n",
      "一个train_batch训练完毕\n",
      "Epoch 10 ，train_loss=0.659，train_acc_I=0.747\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.673，val_accuracy_I=0.738,best_accuracy=0.738\n",
      "一个train_batch训练完毕\n",
      "Epoch 11 ，train_loss=0.643，train_acc_I=0.747\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.668，val_accuracy_I=0.738,best_accuracy=0.738\n",
      "一个train_batch训练完毕\n",
      "Epoch 12 ，train_loss=0.633，train_acc_I=0.747\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.668，val_accuracy_I=0.742,best_accuracy=0.738\n",
      "表示向量保存完毕！\n",
      "一个train_batch训练完毕\n",
      "Epoch 13 ，train_loss=0.626，train_acc_I=0.741\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.675，val_accuracy_I=0.742,best_accuracy=0.742\n",
      "一个train_batch训练完毕\n",
      "Epoch 14 ，train_loss=0.625，train_acc_I=0.741\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.679，val_accuracy_I=0.742,best_accuracy=0.742\n",
      "一个train_batch训练完毕\n",
      "Epoch 15 ，train_loss=0.624，train_acc_I=0.741\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.681，val_accuracy_I=0.742,best_accuracy=0.742\n",
      "一个train_batch训练完毕\n",
      "Epoch 16 ，train_loss=0.623，train_acc_I=0.741\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.678，val_accuracy_I=0.742,best_accuracy=0.742\n",
      "一个train_batch训练完毕\n",
      "Epoch 17 ，train_loss=0.620，train_acc_I=0.741\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.670，val_accuracy_I=0.742,best_accuracy=0.742\n",
      "一个train_batch训练完毕\n",
      "Epoch 18 ，train_loss=0.614，train_acc_I=0.741\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.656，val_accuracy_I=0.742,best_accuracy=0.742\n",
      "一个train_batch训练完毕\n",
      "Epoch 19 ，train_loss=0.605，train_acc_I=0.741\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.641，val_accuracy_I=0.742,best_accuracy=0.742\n",
      "一个train_batch训练完毕\n",
      "Epoch 20 ，train_loss=0.597，train_acc_I=0.741\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.629，val_accuracy_I=0.746,best_accuracy=0.742\n",
      "表示向量保存完毕！\n",
      "一个train_batch训练完毕\n",
      "Epoch 21 ，train_loss=0.587，train_acc_I=0.747\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.617，val_accuracy_I=0.738,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 22 ，train_loss=0.581，train_acc_I=0.743\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.617，val_accuracy_I=0.738,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 23 ，train_loss=0.577，train_acc_I=0.743\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.616，val_accuracy_I=0.738,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 24 ，train_loss=0.572，train_acc_I=0.743\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.612，val_accuracy_I=0.742,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 25 ，train_loss=0.566，train_acc_I=0.747\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.610，val_accuracy_I=0.746,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 26 ，train_loss=0.561，train_acc_I=0.751\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.611，val_accuracy_I=0.746,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 27 ，train_loss=0.556，train_acc_I=0.753\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.615，val_accuracy_I=0.746,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 28 ，train_loss=0.549，train_acc_I=0.753\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.621，val_accuracy_I=0.746,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 29 ，train_loss=0.542，train_acc_I=0.757\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.627，val_accuracy_I=0.746,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 30 ，train_loss=0.535，train_acc_I=0.757\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.632，val_accuracy_I=0.746,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 31 ，train_loss=0.531，train_acc_I=0.757\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.637，val_accuracy_I=0.746,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 32 ，train_loss=0.524，train_acc_I=0.757\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.639，val_accuracy_I=0.738,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 33 ，train_loss=0.519，train_acc_I=0.759\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.638，val_accuracy_I=0.738,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 34 ，train_loss=0.513，train_acc_I=0.762\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.640，val_accuracy_I=0.738,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 35 ，train_loss=0.508，train_acc_I=0.764\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.651，val_accuracy_I=0.738,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 36 ，train_loss=0.502，train_acc_I=0.761\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.660，val_accuracy_I=0.738,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 37 ，train_loss=0.497，train_acc_I=0.761\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.660，val_accuracy_I=0.738,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 38 ，train_loss=0.491，train_acc_I=0.766\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.656，val_accuracy_I=0.727,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 39 ，train_loss=0.485，train_acc_I=0.776\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.658，val_accuracy_I=0.731,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 40 ，train_loss=0.479，train_acc_I=0.785\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.666，val_accuracy_I=0.727,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 41 ，train_loss=0.472，train_acc_I=0.784\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.671，val_accuracy_I=0.727,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 42 ，train_loss=0.466，train_acc_I=0.785\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.671，val_accuracy_I=0.727,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 43 ，train_loss=0.459，train_acc_I=0.789\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.671，val_accuracy_I=0.731,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 44 ，train_loss=0.452，train_acc_I=0.793\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.674，val_accuracy_I=0.731,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 45 ，train_loss=0.444，train_acc_I=0.801\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.681，val_accuracy_I=0.735,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 46 ，train_loss=0.437，train_acc_I=0.807\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.689，val_accuracy_I=0.746,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 47 ，train_loss=0.429，train_acc_I=0.814\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.699，val_accuracy_I=0.746,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 48 ，train_loss=0.421，train_acc_I=0.818\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.713，val_accuracy_I=0.738,best_accuracy=0.746\n",
      "一个train_batch训练完毕\n",
      "Epoch 49 ，train_loss=0.412，train_acc_I=0.828\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.726，val_accuracy_I=0.731,best_accuracy=0.746\n",
      "End of model training\n",
      "test_loss=0.683，test_accuracy_I=0.726\n"
     ]
    }
   ],
   "source": [
    "att=train(\n",
    "    BB_matrix=BB_matrix,\n",
    "    BU_matrix=BU_matrix,\n",
    "    BS_matrix=BS_matrix,\n",
    "    BL_matrix=BL_matrix,\n",
    "    BR_matrix=BR_matrix,\n",
    "    \n",
    "    FB=FB,\n",
    "    FU=FU,\n",
    "    FS=FS,\n",
    "    FL=FL,\n",
    "    FR=FR,\n",
    "          \n",
    "    \n",
    "    epoch_time=50,  \n",
    "    batch_size=1000,\n",
    "          \n",
    "          \n",
    "    id_x_set          =train_idx_for_FB, \n",
    "    id_x_set_for_val  =val_idx_for_FB,\n",
    "    id_x_set_for_test =test_idx_for_FB,\n",
    "\n",
    "  \n",
    "    label_set_I=data_label,\n",
    "    output_size=data_label.shape[1],\n",
    "    dropout=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7524575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.346\n",
      "0.052\n",
      "0.002\n",
      "0.001\n",
      "0.132\n",
      "0.189\n",
      "0.003\n",
      "0.274\n"
     ]
    }
   ],
   "source": [
    "att_1=tf.reduce_sum(att[0],axis=0)/att[0].shape[0]\n",
    "att_weihgt_1=list(np.array(att_1))\n",
    "for i in att_weihgt_1:\n",
    "    print('%.3f' %i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daaf66b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12499995"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(att_weihgt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e57ea6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BUB', 'BLB', 'BSB', 'BRB']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paths_name_BB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8160801a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UB']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paths_name_BU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af18ace2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BS']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paths_name_BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27d09a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LB']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paths_name_BL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2aa6c1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RB']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paths_name_BR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b2fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f419e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
