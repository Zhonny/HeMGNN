{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0502feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy.io as scio\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b81aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d81e66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hone\\AppData\\Local\\Temp/ipykernel_8944/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab8383ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import activations, regularizers, constraints, initializers\n",
    "\n",
    "class Multi_GCNConv(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 activation=lambda x: x,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        super(Multi_GCNConv, self).__init__()\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        \"\"\" GCN has two inputs : [shape(AnCount,An), shape(X_count,X)]\n",
    "        \"\"\"\n",
    "\n",
    "        fdim = input_shape[1][-1]  # feature dim,取特征长度\n",
    "        # 初始化权重矩阵\n",
    "    \n",
    "        \n",
    "        #改成根据输入的邻接矩阵数量，设置等量的weight和bias：\n",
    "        weight_count=input_shape[0][0]\n",
    "        \n",
    "        \n",
    "        self.weight=self.add_weight(name=\"weight\",\n",
    "                               shape=(weight_count,fdim, self.units),\n",
    "                               initializer=self.kernel_initializer,\n",
    "                               trainable=True)\n",
    "       \n",
    "            \n",
    "        self.bias = self.add_weight(name=\"bias\",\n",
    "                               shape=(self.units ),\n",
    "                               initializer=self.bias_initializer,\n",
    "                               trainable=True)\n",
    "      \n",
    "            \n",
    "            \n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\" GCN has two inputs : [An, X]\n",
    "        \"\"\"\n",
    "        #读入的是多个邻接矩阵\n",
    "        self.Multi_An=inputs[0]\n",
    "        self.X = inputs[1]\n",
    "        \n",
    "        #输入的稀疏张量 转 密集张量，因为稀疏张量无法进行3维矩阵乘法\n",
    "        self.Multi_An=tf.sparse.to_dense(self.Multi_An)\n",
    "        \n",
    "        # 计算 XW \n",
    "\n",
    "        h = tf.add(tf.matmul(self.X, self.weight),self.bias) #这一步可以用sparseTensor，不知道有没有必要\n",
    "            \n",
    "        # 计算 AXW ,A矩阵已经处理过的\n",
    "        \n",
    "        output = tf.matmul(self.Multi_An, h)\n",
    "\n",
    "        #output = tf.sparse.sparse_dense_matmul(self.An, h)\n",
    "     \n",
    "\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f20fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 att_size,\n",
    "                 activation=lambda x: x,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',):\n",
    "        super(Self_Attention, self).__init__()\n",
    "        self.activation = activations.get(activation)\n",
    "        self.att_size=att_size\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        weight_att_shape_x=input_shape[-1]\n",
    "        weight_att_shape_y=self.att_size\n",
    "        \n",
    "        self.weight_att=self.add_weight(name=\"weight_to_att\",\n",
    "                               shape=(weight_att_shape_x, weight_att_shape_y),\n",
    "                               initializer=self.kernel_initializer, \n",
    "                               trainable=True)\n",
    "        \n",
    "        self.bias=self.add_weight(name=\"bias\",\n",
    "                               shape=(weight_att_shape_y,),\n",
    "                               initializer=self.bias_initializer,\n",
    "                               trainable=True)\n",
    "        \n",
    "        self.weight_att_u=self.add_weight(name=\"weight_to_att_u\",\n",
    "                               shape=(weight_att_shape_y, 1),\n",
    "                               initializer=self.kernel_initializer,\n",
    "                               trainable=True)\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        self.X = inputs\n",
    "        \n",
    "        WX=tf.matmul(self.X,self.weight_att)\n",
    "        V=tf.tanh(tf.add(WX,self.bias))\n",
    "        VU=tf.matmul(V,self.weight_att_u)\n",
    "        alphas=tf.nn.softmax(VU,axis=0) \n",
    "        \n",
    "        outputs=tf.multiply(alphas,self.X)\n",
    "        outputs_att=tf.reduce_sum(outputs,axis=0) \n",
    "        \n",
    "#         print('inputs.shape=',inputs.shape)\n",
    "#         print('VU.shape=',VU.shape)\n",
    "#         print('alphas.shape=',alphas.shape)\n",
    "        alphas_new=tf.transpose(tf.squeeze(alphas, axis=-1)) #需要改变attention形状\n",
    "\n",
    "        return outputs_att,alphas_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11460a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, regularizers, constraints, initializers\n",
    "\n",
    "class Multi_GC_MC_Conv(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 activation=lambda x: x,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        super(Multi_GC_MC_Conv, self).__init__()\n",
    "\n",
    "        self.units = units #要降成的维度\n",
    "        self.activation = activations.get(activation)\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #print(input_shape)\n",
    "\n",
    "        \"\"\"\n",
    "        GC-MC has three inputs : [An,X，Z]\n",
    "        X is self_feature,Z is another node type feature\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        ###   目标为XW1+(CA)ZW2    ###\n",
    "        \n",
    "        \n",
    "         \n",
    "        ###   这里开始为设置XW1的参数\n",
    "        \n",
    "        \n",
    "        fdim_X = input_shape[1][-1]  # feature dim,取X特征长度\n",
    "        weight_count=input_shape[0][0]   #改成根据输入的邻接矩阵数量，设置等量的weight和bias：\n",
    "        \n",
    "        self.weight_1=self.add_weight(name=\"weight_1\",\n",
    "                               shape=(weight_count,fdim_X, self.units),\n",
    "                               initializer=self.kernel_initializer,\n",
    "                               trainable=True)\n",
    "       \n",
    "        self.bias_1 = self.add_weight(name=\"bias_1\",\n",
    "                               shape=(self.units ),\n",
    "                               initializer=self.bias_initializer,\n",
    "                               trainable=True)\n",
    "        \n",
    "        ###   这里设置(CA)ZW2的参数\n",
    "        \n",
    "        fdim_Z=input_shape[2][-1]\n",
    "        \n",
    "        self.weight_2=self.add_weight(name=\"weight_2\",\n",
    "                               shape=(weight_count,fdim_Z, self.units), #fdim_Z这个不对，不是12100，是5403\n",
    "                               initializer=self.kernel_initializer,\n",
    "                               trainable=True)\n",
    "       \n",
    "        self.bias_2 = self.add_weight(name=\"bias_2\",\n",
    "                               shape=(self.units ),\n",
    "                               initializer=self.bias_initializer,\n",
    "                               trainable=True)\n",
    "      \n",
    "            \n",
    "            \n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\" GC-MC has three inputs : [An, X，Z]\n",
    "        \"\"\"\n",
    "        self.Multi_An=inputs[0]\n",
    "        self.X = inputs[1]\n",
    "        self.Z = inputs[2]   \n",
    "        \n",
    "        #输入的稀疏张量 转 密集张量，因为稀疏张量无法进行3维矩阵乘法\n",
    "        self.Multi_An=tf.sparse.to_dense(self.Multi_An)\n",
    "        \n",
    "        ### 计算 XW1\n",
    "        \n",
    "        #print('self.X, self.weight_1',self.X,self.weight_1)\n",
    "\n",
    "        XW1 = tf.add(tf.matmul(self.X, self.weight_1),self.bias_1) #这一步可以用sparseTensor，不知道有没有必要\n",
    "        \n",
    "                \n",
    "        \n",
    "        ### 计算 (CA)ZW2\n",
    "        \n",
    "        #计算C，不需要了，在数据预处理部分已经计算了CA\n",
    "        pass\n",
    "        \n",
    "        #计算ZW2\n",
    " \n",
    "       \n",
    "        \n",
    "        ZW2 = tf.add(tf.matmul(self.Z, self.weight_2),self.bias_2) #这一步可以用sparseTensor，不知道有没有必要\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 计算 (CA)T ZW2\n",
    "\n",
    "        \n",
    "        if self.Multi_An.shape[-1]!=ZW2.shape[1]:\n",
    "            \n",
    "            CA_T=tf.transpose(self.Multi_An,perm=[0,2,1])\n",
    "            #print('需要转置，CA_T.shape=',CA_T.shape)\n",
    "            output1= tf.matmul(CA_T,ZW2) \n",
    "        else:\n",
    "\n",
    "            output1= tf.matmul(self.Multi_An,ZW2)  \n",
    "\n",
    "\n",
    "        \n",
    "        #计算 XW1+(CA)T ZW2\n",
    "      \n",
    "        output=tf.add(XW1,output1)        \n",
    "               \n",
    "#         if self.use_bias:\n",
    "#             output = tf.nn.bias_add(output, self.bias) \n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36aa8229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_graph(adj):\n",
    "    # _A = A + I\n",
    "    _adj = adj + sp.eye(adj.shape[0])\n",
    "    # _dseq：各个节点的度构成的列表\n",
    "    _dseq = _adj.sum(1).A1\n",
    "    # 构造开根号的度矩阵\n",
    "    _D_half = sp.diags(np.power(_dseq, -0.5))\n",
    "    # 计算标准化的邻接矩阵, @ 表示矩阵乘法\n",
    "    adj_normalized = _D_half @ _adj @ _D_half\n",
    "    return adj_normalized.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "742fac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 稀疏矩阵转稀疏张量\n",
    "def sp_matrix_to_sp_tensor(M):\n",
    "    if not isinstance(M, sp.csr.csr_matrix):\n",
    "        M = M.tocsr()\n",
    "    # 获取非0元素坐标\n",
    "    row, col = M.nonzero()\n",
    "    # SparseTensor参数：二维坐标数组，数据，形状\n",
    "    X = tf.SparseTensor(np.mat([row, col]).T, M.data, M.shape)\n",
    "    X = tf.cast(X, tf.float32)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "541490ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_adj_matrix_to_sparse_3D(Paths_name,data):\n",
    "    \n",
    "    adj_matrix_list=[]\n",
    "    for name in Paths_name:\n",
    "        #mat=data.get(name)\n",
    "        mat=data[name]\n",
    "        \n",
    "        #如果是方阵，令对角线元素为零, 处理成DAD\n",
    "        if mat.shape[0]==mat.shape[1]:\n",
    "            #mat[np.eye(mat.shape[0],dtype=bool)]=0\n",
    "            mat=sp.csr_matrix(mat)\n",
    "            mat=preprocess_graph(mat) #A to DAD\n",
    "            \n",
    "        else: #非方阵，处理成CA\n",
    "            #统一矩阵形式，N x M, N>=m \n",
    "            if mat.shape[0] < mat.shape[1]:\n",
    "                mat=np.transpose(mat)\n",
    "                \n",
    "            #计算C\n",
    "            sum_row=np.sum(mat,axis=1)\n",
    "            sum_column=np.sum(mat,axis=0)\n",
    "            \n",
    "            #sum_row=np.expand_dims(sum_row,axis=0)\n",
    "            #sum_column=np.expand_dims(sum_column,axis=0)\n",
    "            \n",
    "            #sum_row=np.transpose(sum_row,axes=(1,0))\n",
    "            \n",
    "            \n",
    "            C=sum_row @ sum_column\n",
    "            \n",
    "            #C矩阵预处理,\n",
    "            #C[C==0]=1 \n",
    "            C=1./np.sqrt(C) \n",
    "            C=np.nan_to_num(C,0) #将nan转成0\n",
    "            #C=C.T\n",
    "         \n",
    "            \n",
    "   \n",
    "            #CA\n",
    "            mat=mat.todense()\n",
    "   \n",
    "        \n",
    "            mat=np.multiply(C,mat)\n",
    "            \n",
    "            mat=sp.csr_matrix(mat)\n",
    "           \n",
    "    \n",
    "        mat=sp_matrix_to_sp_tensor(mat)       #稀疏矩阵 转 稀疏张量\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        mat=tf.sparse.expand_dims(mat,axis=0) #2D 扩 3D\n",
    "        \n",
    "\n",
    "        adj_matrix_list.append(mat)\n",
    "       \n",
    "    sparse_matrix_3D=tf.sparse.concat(0,adj_matrix_list) #3D张量堆叠\n",
    "    print('矩阵处理完毕')\n",
    "    return sparse_matrix_3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c894174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_GCN_GCMC_Model(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 Muilt_AA,\n",
    "                 Muilt_AP,\n",
    "                 Muilt_AT,\n",
    "                 Muilt_AC,\n",
    "             \n",
    "                 \n",
    "                 Muilt_FA,\n",
    "                 Muilt_FP,\n",
    "                 Muilt_FT,\n",
    "                 Muilt_FC,\n",
    "    \n",
    "                 output_size=3): \n",
    "\n",
    "        # 调用父类__init__()方法\n",
    "        super(Multi_GCN_GCMC_Model, self).__init__()\n",
    "        \n",
    "        self.Muilt_AA=Muilt_AA\n",
    "        self.Muilt_AP=Muilt_AP\n",
    "        self.Muilt_AT=Muilt_AT\n",
    "        self.Muilt_AC=Muilt_AC\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.Muilt_FA =Muilt_FA\n",
    "        self.Muilt_FP =Muilt_FP\n",
    "        self.Muilt_FT =Muilt_FT\n",
    "        self.Muilt_FC =Muilt_FC\n",
    "     \n",
    "        \n",
    "\n",
    "        \n",
    "        #同质图\n",
    "        self.Multi_GCN_layer_1_for_AA = Multi_GCNConv(64,activation=tf.keras.activations.relu)\n",
    "\n",
    "\n",
    "        \n",
    "        #二部图\n",
    "#         self.Multi_GCMC_layer_1_for_AP = Multi_GC_MC_Conv(64,activation=tf.keras.activations.relu)\n",
    "#         self.Multi_GCMC_layer_1_for_AT = Multi_GC_MC_Conv(64,activation=tf.keras.activations.relu)\n",
    "#         self.Multi_GCMC_layer_1_for_AC = Multi_GC_MC_Conv(64,activation=tf.keras.activations.relu)\n",
    "   \n",
    " \n",
    "        \n",
    "        \n",
    "        #注意力\n",
    "        self.att_layer = Self_Attention(32)\n",
    "\n",
    "    \n",
    "        #Decoder部分\n",
    "        self.dense2=tf.keras.layers.Dense(output_size)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def call(self,input_x_id, training=False,dropout=0.):\n",
    "        # 输入数据\n",
    "        \n",
    "        #同质网络I 编码过程\n",
    "   \n",
    "        _h1_for_A = self.Multi_GCN_layer_1_for_AA([self.Muilt_AA, self.Muilt_FA])\n",
    "        if training:\n",
    "            _h1_for_A = tf.nn.dropout(_h1_for_A, dropout) \n",
    "\n",
    "            \n",
    "\n",
    "#         #二部图AP 编码过程\n",
    "  \n",
    "#         _h2_for_A = self.Multi_GCMC_layer_1_for_AP([self.Muilt_AP,  self.Muilt_FA, self.Muilt_FP])\n",
    "        \n",
    "#         if training:\n",
    "#             _h2_for_A = tf.nn.dropout(_h2_for_A, dropout)\n",
    "            \n",
    "        \n",
    "#         #二部图AT 编码过程\n",
    "\n",
    "#         _h3_for_A = self.Multi_GCMC_layer_1_for_AT([self.Muilt_AT,  self.Muilt_FA, self.Muilt_FT])\n",
    "        \n",
    "#         if training:\n",
    "#             _h3_for_A = tf.nn.dropout(_h3_for_A, dropout)\n",
    "            \n",
    "            \n",
    "#         #二部图AC 编码过程\n",
    "   \n",
    "#         _h4_for_A = self.Multi_GCMC_layer_1_for_AC([self.Muilt_AC,  self.Muilt_FA, self.Muilt_FC])\n",
    "        \n",
    "#         if training:\n",
    "#             _h4_for_A = tf.nn.dropout(_h4_for_A, dropout)\n",
    "            \n",
    "            \n",
    "  \n",
    "          \n",
    "        \n",
    "       \n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "        #注意力需要最后连接，同质图和二部图的表示向量，\n",
    "\n",
    "        #_h6_for_A = tf.concat([ _h1_for_A,_h2_for_A,_h3_for_A,_h4_for_A],axis=0) \n",
    "\n",
    "        _h6_for_A = tf.concat([ _h1_for_A,],axis=0) \n",
    "        _h7_for_A,att_for_A=self.att_layer(_h6_for_A) \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        #解码层\n",
    "\n",
    "        output=self.dense2(_h7_for_A) \n",
    "        \n",
    "        \n",
    "        \n",
    "        output_list=[]\n",
    "        for idx in input_x_id:\n",
    "            output_list.append(output[idx]) #返回指定id的所有向量\n",
    "            \n",
    "\n",
    "        att=[att_for_A,]\n",
    "      \n",
    "        \n",
    "        \n",
    "        embedding=_h7_for_A\n",
    "\n",
    "            \n",
    "            \n",
    "        return output_list,att,embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d4a0e",
   "metadata": {},
   "source": [
    "# 加入数据测试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b07cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取文件夹下所有文件名\n",
    "import os\n",
    "path=r'C:\\Users\\Hone\\Desktop\\Experimental_code\\DBLP_2\\计算的各个元路径矩阵'\n",
    "name_list=[]\n",
    "filelist=os.listdir(path)\n",
    "for filename in filelist:\n",
    "    name_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965bad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#取出矩阵的名字\n",
    "matrix_name_list=[]\n",
    "for name in name_list:\n",
    "    matrix_name_list.append(name.split('.')[0].split('_matrix_')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f15551b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['APA',\n",
       " 'APCP',\n",
       " 'APCPA',\n",
       " 'APCPT',\n",
       " 'APTP',\n",
       " 'APTPA',\n",
       " 'APTPC',\n",
       " 'CPA',\n",
       " 'CPAPA',\n",
       " 'PA',\n",
       " 'PAPA',\n",
       " 'TPA',\n",
       " 'TPAPA',\n",
       " 'TPTPA']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3fff060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matrix_from_file(path,key_name):\n",
    "    filelist=os.listdir(path)\n",
    "    \n",
    "    for name in filelist:\n",
    "        if key_name==name.split('.')[0].split('_matrix_')[1]:\n",
    "            Matrix = sp.load_npz(path+'\\\\'+name) #读取\n",
    "            return Matrix\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "\n",
    "    print('cant find %s in %s' %key_name %path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8a19964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#需要将矩阵按照类型进行区分，放进不同的dict\n",
    "data_AA={}\n",
    "data_AP={}\n",
    "data_AT={}\n",
    "data_AC={}\n",
    "\n",
    "\n",
    "\n",
    "for pathname in matrix_name_list:\n",
    "    if pathname[0]==pathname[-1]: #同质邻接矩阵\n",
    "        data_AA[pathname]=find_matrix_from_file(path,pathname) #输入目标位置文件，与指定文件名，返回该稀疏矩阵\n",
    "    elif pathname[0]=='P' or pathname[-1]=='P':\n",
    "        data_AP[pathname]=find_matrix_from_file(path,pathname) \n",
    "    elif pathname[0]=='T' or pathname[-1]=='T':\n",
    "        data_AT[pathname]=find_matrix_from_file(path,pathname) \n",
    "    elif pathname[0]=='C' or pathname[-1]=='C':\n",
    "        data_AC[pathname]=find_matrix_from_file(path,pathname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc972fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paths_name_AA=[keyword for keyword in data_AA]\n",
    "Paths_name_AP=[keyword for keyword in data_AP]\n",
    "Paths_name_AT=[keyword for keyword in data_AT]\n",
    "Paths_name_AC=[keyword for keyword in data_AC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0efc9fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths_name_AA=[  'APCPA', 'APTPA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89343357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths_name_AP=['APCP', 'APTP',  'PAPA']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e3d26f",
   "metadata": {},
   "source": [
    "# 下面是矩阵预处理过程，包括DAD,CA，再转成3维数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93020bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "矩阵处理完毕\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in sqrt\n",
      "C:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "矩阵处理完毕\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in sqrt\n",
      "C:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in sqrt\n",
      "C:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "矩阵处理完毕\n",
      "矩阵处理完毕\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in sqrt\n",
      "C:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    }
   ],
   "source": [
    "#处理I矩阵，2维变3维,变sparseTensor\n",
    "AA_matrix = preprocess_adj_matrix_to_sparse_3D(Paths_name_AA,data_AA) #输入的是list和dict数据\n",
    "AP_matrix = preprocess_adj_matrix_to_sparse_3D(Paths_name_AP,data_AP)\n",
    "AT_matrix = preprocess_adj_matrix_to_sparse_3D(Paths_name_AT,data_AT)\n",
    "AC_matrix = preprocess_adj_matrix_to_sparse_3D(Paths_name_AC,data_AC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb746be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337ba1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52f6cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FA=np.eye(4057)\n",
    "FP=np.eye(14376)\n",
    "FT=np.eye(8920)\n",
    "FC=np.eye(20)\n",
    "\n",
    "\n",
    "FA=np.expand_dims(FA,axis=0)\n",
    "FP=np.expand_dims(FP,axis=0)\n",
    "FT=np.expand_dims(FT,axis=0)\n",
    "FC=np.expand_dims(FC,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78021b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入训练id集合\n",
    "import random\n",
    "\n",
    "index_list_for_FA=[i for i in range(4057)] #需要随机化\n",
    "random.shuffle(index_list_for_FA) #index乱序\n",
    "    \n",
    "a_part=int(len(index_list_for_FA)/10)\n",
    "train_idx_for_FA = index_list_for_FA[:int(a_part*2)]  # 2 1 7\n",
    "val_idx_for_FA = index_list_for_FA[ int(a_part*2)+1:int(a_part*3)]\n",
    "test_idx_for_FA = index_list_for_FA[ int(a_part*3)+1:len(index_list_for_FA)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edd30b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=Multi_GCN_GCMC_Model(AA_matrix,\n",
    "#                            AP_matrix,\n",
    "#                            AT_matrix,\n",
    "#                            AC_matrix,\n",
    "                      \n",
    "                           \n",
    "#                            FA,\n",
    "#                            FP,\n",
    "#                            FT,\n",
    "#                            FC,\n",
    "                        \n",
    "                           \n",
    "#                            output_size=4\n",
    "#                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcb66b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output,att,embedding=model(index_list_for_FA,training=True,dropout=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "726ecf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a3ef64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#标签\n",
    "data_label=sp.load_npz(r'C:\\Users\\Hone\\Desktop\\Experimental_code\\DBLP_2\\sparse_matrix_author_label.npz') #读取\n",
    "data_label=data_label.todense()\n",
    "data_label=np.array(data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20834df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4057, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13e14421",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "loss_func=tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "#这个loss函数还不够全，还得加入L2正则化\n",
    "\n",
    "\n",
    "train_loss=tf.keras.metrics.Mean('train_loss') #计算loss的均值\n",
    "train_acc_I=tf.keras.metrics.CategoricalAccuracy('train_accuracy_I')\n",
    "\n",
    "\n",
    "test_loss=tf.keras.metrics.Mean('test_loss')\n",
    "test_acc_I=tf.keras.metrics.CategoricalAccuracy('test_accuracy_I')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa338531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model,id_x,labels_x,dropout=0.):  #一个set为一个batch的量，\n",
    "    #tf.GradientTape()记录运算过程的变量梯度\n",
    "    \n",
    "    with tf.GradientTape() as t:\n",
    "        output_list,att,embedding=model(input_x_id=id_x,training=True,dropout=dropout)    #计算一个batch的预测值   #!!!\n",
    "     \n",
    "\n",
    "\n",
    "        loss_step=loss_func(labels_x,output_list)   #计算一个batch的损失函数\n",
    "            \n",
    "        \n",
    " \n",
    "    grads=t.gradient(loss_step,model.trainable_variables)  #更新变量的梯度\n",
    "    \n",
    "    \n",
    "    #利用跟新的梯度计算变量，model.trainable_variables表示网络中所有的变量\n",
    " \n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_variables))  \n",
    "    train_loss(loss_step)  #传入计算一个epoch累计loss函数中\n",
    "    train_acc_I(labels_x,output_list) #传入计算一个epoch累计accuracy函数中\n",
    "    \n",
    "    return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00c0ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model,id_x,labels_x): \n",
    "    output_list,att,embedding=model(input_x_id=id_x,training=False,dropout=0.) \n",
    "    loss_step=loss_func(labels_x,output_list)   #计算一个batch的损失函数\n",
    "    test_loss(loss_step)\n",
    "    test_acc_I(labels_x,output_list) #传入计算一个epoch累计accuracy函数中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a016980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_by_n(list_collection, n):\n",
    "    \"\"\"\n",
    "    将集合均分，每份n个元素\n",
    "    :param list_collection:\n",
    "    :param n:\n",
    "    :return:返回的结果为评分后的每份可迭代对象\n",
    "    \"\"\"\n",
    "    for i in range(0, len(list_collection), n):\n",
    "        yield list_collection[i: i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b8002fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "          AA_matrix,\n",
    "          AP_matrix,\n",
    "          AT_matrix,\n",
    "          AC_matrix,\n",
    "       \n",
    "    \n",
    "          FA,\n",
    "          FP,\n",
    "          FT,\n",
    "          FC,\n",
    "     \n",
    "    \n",
    "          epoch_time,  \n",
    "          batch_size,\n",
    "          \n",
    "          \n",
    "          id_x_set, \n",
    "          id_x_set_for_val,\n",
    "          id_x_set_for_test,\n",
    "          \n",
    "  \n",
    "          label_set_I,\n",
    "          output_size=3,\n",
    "          dropout=0.\n",
    "         ): #需要给要训练的集合和验证集合\n",
    "    \n",
    "    model=Multi_GCN_GCMC_Model(AA_matrix,\n",
    "                               AP_matrix,\n",
    "                               AT_matrix,\n",
    "                               AC_matrix,\n",
    "                         \n",
    "                               \n",
    "                               FA,\n",
    "                               FP,\n",
    "                               FT,\n",
    "                               FC,\n",
    "         \n",
    "                               \n",
    "                               output_size=output_size\n",
    "                               )\n",
    "\n",
    "    \n",
    "    best_acc_result=0\n",
    "    for epoch in range(epoch_time):\n",
    "        \n",
    "        #training\n",
    "        batch_idx_train=[] #获取一个batch的数据\n",
    "        for i in split_list_by_n(id_x_set,batch_size): #split_list_by_n这个函数\n",
    "            batch_idx_train.append(i)  \n",
    "                     \n",
    "        #创建batch_label矩阵\n",
    "\n",
    "        \n",
    "        for a_batch_x in batch_idx_train: \n",
    "            \n",
    "            batch_label_list_x=[]\n",
    "\n",
    "            for id_x in a_batch_x:\n",
    "                batch_label_list_x.append(label_set_I[id_x]) \n",
    "                \n",
    "    \n",
    "\n",
    "            att=train_step(model,\n",
    "                           id_x=a_batch_x,\n",
    "                           labels_x=batch_label_list_x,\n",
    "                           dropout=dropout) \n",
    "            print('一个train_batch训练完毕')\n",
    "                           \n",
    "        print('Epoch {} ，train_loss={:.3f}，train_acc_I={:.3f}'.format(epoch,\n",
    "                                                                        train_loss.result(),\n",
    "                                                                        train_acc_I.result())) \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #validating\n",
    "        batch_idx_val=[] #获取一个batch的数据\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in split_list_by_n(id_x_set_for_val,batch_size):\n",
    "            batch_idx_val.append(i)\n",
    "            \n",
    "\n",
    "            \n",
    "        for a_batch_x in batch_idx_val:\n",
    "            batch_label_list_x=[]\n",
    "            \n",
    "            for id_x in a_batch_x:\n",
    "                batch_label_list_x.append(label_set_I[id_x]) #获取一个batch的所有label\n",
    "\n",
    "                \n",
    "            \n",
    "            test_step(model,id_x=a_batch_x, labels_x=batch_label_list_x)\n",
    "            print('一个val_batch验证完毕')\n",
    "            \n",
    "        print('val_loss={:.3f}，val_accuracy_I={:.3f},best_accuracy={:.3f}'.format(test_loss.result(),test_acc_I.result(),best_acc_result)\n",
    "             )\n",
    "        \n",
    "        ####################################################################################\n",
    "        #保存结果最佳的embedding\n",
    "        test_acc=float(test_acc_I.result())\n",
    "        if test_acc>best_acc_result:\n",
    "            \n",
    "            best_acc_result=test_acc\n",
    "           \n",
    "            \n",
    "            #这个阈值保存满足条件的embedding\n",
    "            if best_acc_result>0.7:\n",
    "                \n",
    "                #需要返回，train数据载入得到的att\n",
    "                att=train_step(model,\n",
    "                    id_x=id_x_set,\n",
    "                    labels_x=label_set_I[id_x_set]) \n",
    "    \n",
    "                #输出test数据集的embedding \n",
    "                #输入test数据集，输出test数据集的embedding\n",
    "                _,att_test,embedding=model(input_x_id=id_x_set_for_test,\n",
    "                          training=False) \n",
    "                #还需要一个对应的label矩阵\n",
    "                #从label_set_I中取出对应id的值，重新组成一个新的矩阵\n",
    "                embedding_list=[]\n",
    "                new_label_matrix_list=[]\n",
    "                for i in id_x_set_for_test:\n",
    "                    new_label_matrix_list.append(label_set_I[i])\n",
    "                    embedding_list.append(embedding[i])\n",
    "                    new_label_matrix=np.array(new_label_matrix_list)\n",
    "    \n",
    "                #保存embedding和对应的label矩阵\n",
    "                np.save(r'C:\\Users\\Hone\\Desktop\\My_model-DATA\\DBLP\\embedding_of_test_data_GCN_heo',np.array(embedding_list))\n",
    "                np.save(r'C:\\Users\\Hone\\Desktop\\My_model-DATA\\DBLP\\label_matrix_of_test_data_GCN_heo',new_label_matrix)\n",
    "                print('表示向量保存完毕！')\n",
    "\n",
    "        train_loss.reset_states()\n",
    "        train_acc_I.reset_states()\n",
    "\n",
    "        \n",
    "        test_loss.reset_states()\n",
    "        test_acc_I.reset_states()\n",
    "        ####################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "    #循环结束,对test数据验证\n",
    "    batch_idx_test = []  # 获取一个batch的数据\n",
    "\n",
    "    \n",
    "    for i in split_list_by_n(id_x_set_for_test, batch_size):\n",
    "        batch_idx_test.append(i)\n",
    "\n",
    "    \n",
    "\n",
    "    for a_batch_x in batch_idx_test:\n",
    "        batch_label_list_x = []\n",
    "\n",
    "        \n",
    "        for id_x in a_batch_x:\n",
    "            batch_label_list_x.append(label_set_I[id_x])  # 获取一个batch的所有label\n",
    "    \n",
    "        \n",
    "\n",
    "        test_step(model,id_x=a_batch_x,labels_x=batch_label_list_x)\n",
    "    print('End of model training\\n'\n",
    "          'test_loss={:.3f}，test_accuracy_I={:.3f}'.format(test_loss.result(),\n",
    "                                                          test_acc_I.result()))\n",
    "    \n",
    "\n",
    "    test_loss.reset_states()\n",
    "    test_acc_I.reset_states()\n",
    "          \n",
    "    return att_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bee1255e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一个train_batch训练完毕\n",
      "Epoch 0 ，train_loss=1.387，train_acc_I=0.241\n",
      "一个val_batch验证完毕\n",
      "val_loss=1.369，val_accuracy_I=0.488,best_accuracy=0.000\n",
      "一个train_batch训练完毕\n",
      "Epoch 1 ，train_loss=1.338，train_acc_I=0.498\n",
      "一个val_batch验证完毕\n",
      "val_loss=1.336，val_accuracy_I=0.295,best_accuracy=0.488\n",
      "一个train_batch训练完毕\n",
      "Epoch 2 ，train_loss=1.261，train_acc_I=0.484\n",
      "一个val_batch验证完毕\n",
      "val_loss=1.246，val_accuracy_I=0.500,best_accuracy=0.488\n",
      "一个train_batch训练完毕\n",
      "Epoch 3 ，train_loss=1.107，train_acc_I=0.633\n",
      "一个val_batch验证完毕\n",
      "val_loss=1.095，val_accuracy_I=0.728,best_accuracy=0.500\n",
      "表示向量保存完毕！\n",
      "一个train_batch训练完毕\n",
      "Epoch 4 ，train_loss=0.617，train_acc_I=0.905\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.700，val_accuracy_I=0.824,best_accuracy=0.728\n",
      "表示向量保存完毕！\n",
      "一个train_batch训练完毕\n",
      "Epoch 5 ，train_loss=0.227，train_acc_I=0.973\n",
      "一个val_batch验证完毕\n",
      "val_loss=0.616，val_accuracy_I=0.842,best_accuracy=0.824\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8944/3851493125.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mlabel_set_I\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0moutput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8944/1132818733.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(AA_matrix, AP_matrix, AT_matrix, AC_matrix, FA, FP, FT, FC, epoch_time, batch_size, id_x_set, id_x_set_for_val, id_x_set_for_test, label_set_I, output_size, dropout)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \u001b[1;31m#保存embedding和对应的label矩阵\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\Hone\\Desktop\\My_model-DATA\\DBLP\\embedding_of_test_data_GCN_heo'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\Hone\\Desktop\\My_model-DATA\\DBLP\\label_matrix_of_test_data_GCN_heo'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_label_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'表示向量保存完毕！'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   7217\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_limit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7218\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7219\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7220\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7221\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1094\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m   1051\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m         \u001b[0mvar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1053\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1094\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m   1223\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1225\u001b[1;33m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[0;32m   1226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1227\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\TF2.0\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m  10665\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"StridedSlice\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"begin_mask\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10666\u001b[0m         \u001b[0mbegin_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"end_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ellipsis_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10667\u001b[1;33m         \"new_axis_mask\", new_axis_mask, \"shrink_axis_mask\", shrink_axis_mask)\n\u001b[0m\u001b[0;32m  10668\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10669\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "att=train(\n",
    "    AA_matrix=AA_matrix,\n",
    "    AP_matrix=AP_matrix,\n",
    "    AT_matrix=AT_matrix,\n",
    "    AC_matrix=AC_matrix,\n",
    " \n",
    "    \n",
    "    FA=FA,\n",
    "    FP=FP,\n",
    "    FT=FT,\n",
    "    FC=FC,\n",
    "\n",
    "   \n",
    "    epoch_time=10,  \n",
    "    batch_size=1000,\n",
    "          \n",
    "          \n",
    "    id_x_set          =train_idx_for_FA, \n",
    "    id_x_set_for_val  =val_idx_for_FA,\n",
    "    id_x_set_for_test =test_idx_for_FA,\n",
    "\n",
    "  \n",
    "    label_set_I=data_label,\n",
    "    output_size=data_label.shape[1],\n",
    "    dropout=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=Paths_name_AA+Paths_name_AP+Paths_name_AT+Paths_name_AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7524575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_1=tf.reduce_sum(att[0],axis=0)/att[0].shape[0]\n",
    "att_weihgt_1=list(np.array(att_1))\n",
    "for i in att_weihgt_1:\n",
    "    print('%.3f' %i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784916fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存元路径注意力权重\n",
    "# att=np.array(att_weihgt_1)\n",
    "# np.save(r'C:\\Users\\Hone\\Desktop\\Experimental_code\\DBLP_2\\att_weight',att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdda532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
